{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "<b> 1. Import data from a file. </b><br/>\n",
    "<b> 2. Take Column which contains comments/text data from dataframe. </b> <br/>\n",
    "<b> 3. Clean all the text by performing following sub-steps: </b> <br/>\n",
    "        &nbsp; 3.0 Tokenization — convert sentences to words <br/>\n",
    "        &nbsp; 3.1 Remove punctuations and symbols <br/>\n",
    "        &nbsp; 3.2 Remove stop words and normalize by converting into upper or lower<br/>\n",
    "        &nbsp; 3.3 Stemming — words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix. <br/>\n",
    "        &nbsp; 3.4 Lemmatization — Another approach to remove inflection by determining the part of speech and utilizing detailed database of the language. <br/>\n",
    "        &nbsp; 3.5 Part-of-Speech (POS) Tagging — You can decide whether to remove some words based on their tags like Verbs, Adjectives, Pro-nouns etc. You can use this step as a filter to remove extra words. <br/> \n",
    "<b> 4. Create a bag-of-words (BOW) model with N-grams (Uni,bi, tri etc): </b><br/>\n",
    "        &nbsp; 4.1 Use Count Vectorizer (How many times a word occur in a document or row)<br/>\n",
    "        &nbsp; 4.2 Use Tf-IDF vectorizer (word frequency (TF) / Inverse Document Frequency (Total No of documents / Documents containing t term))<br/>\n",
    "<b>NOTE:</b> <br/>\n",
    "        &nbsp; <i>a.</i> If you have an output variable, then use appropriate machine learning model <br/>\n",
    "            &nbsp; &nbsp; (Usually all NLP related problems are classification problems known as Text Classification) <br/>\n",
    "        &nbsp; <i>b.</i> If you do not have a output variable (like you want to do sentiment analysis), then:<br/>\n",
    "            &nbsp; &nbsp; <i>b.1)</i> Use pre-trained model (from TextBlob library) <br/>\n",
    "            &nbsp; &nbsp; <i>b.2)</i> Use clustering machine learning model (Clustering algorithm like K-means) <br/>        \n",
    "<b> 5. Split data into training and test data </b><br/>\n",
    "<b> 6. Give it to machine learning </b><br/>\n",
    "<b> THE END </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 libraries that are being used most frequently\n",
    "# 1) NLTK\n",
    "# 2) Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re # Regular expression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stop words from nltk \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Create an object of lemmatizer \n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a corpus\n",
    "corpus=[] # To store cleaned text/comments\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    # str1 = \"Hello, my name is Vishwa Patel troubled @ $ 123.\"\n",
    "    str1 = str(dataset[\"Review\"].iloc[i])\n",
    "    \n",
    "    # Step1: Remove punctuations and symbols\n",
    "    str1 = re.sub('[^a-zA-Z0-9]', ' ', str1)\n",
    "    # print(\"Punctuations removed: \",str1)\n",
    "\n",
    "    str1 = \" \".join(str1.split()) # removing extra spaces from the text\n",
    "\n",
    "    # Step-2: To upper or lower case (Normalize text data)\n",
    "    str1 = str1.lower()\n",
    "    \n",
    "    # Step-3: Remove stop words\n",
    "    str1 = [i for i in str1.split() if i not in set(stopwords.words('english'))]\n",
    "    str1 = \" \".join(str1)\n",
    "    # print(str1)\n",
    "\n",
    "\n",
    "    # Step-4: Lemmatizing each words\n",
    "    str1 = \" \".join([lem.lemmatize(i) for i in str1.split()])\n",
    "    \n",
    "    # Step-5: Append clean text in corpus list\n",
    "    corpus.append(str1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Bag-of_Words model with corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range= (1,1), max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform data \n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to view Count Vectorized Matrix with their column names then use following code\n",
    "df = pd.DataFrame(X, columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 1].values\n",
    "# Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a machine learning model (Classification)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data into model\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the results using X_test\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48 49]\n",
      " [14 89]]\n"
     ]
    }
   ],
   "source": [
    "# Measure the performane of the model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
